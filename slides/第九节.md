# 第九课-自制深度学习推理框架-实现Yolov5网络的推理

> 作者：傅莘莘
>
> 主项目：https://github.com/zjhellofss/KuiperInfer 欢迎大家点赞和PR.
>
> 课程代码：https://github.com/zjhellofss/kuiperdatawhale/course9

![](https://i.imgur.com/qO2yKaH.jpg)

## Yolov5中的预处理


预处理的作用与上一节中的`ResNet`网络中的预处理函数类似，主要包括以下几个步骤:

1. 图像缩放
2. 图像补边
3. 颜色空间转换
4. 图像归一化
5. 将`RGBRGBRGB`的像素格式转换为`RRRGGGBBB`的像素格式，也就是将像素排布从`HWC`到`CHW`。

Yolov5定义了一个`PreProcessImage`函数来完成图像的预处理，在这个函数中依次调用了图像缩放、补边等如上的图像预处理流程。

### 图像缩放和补边

<img src="https://img-blog.csdnimg.cn/402d3a22b1d6439393bf9a14e700d196.jpeg" alt="dem113" style="zoom: 80%;" />

如上图所示，在`YOLOv5`中，补边（padding）是一种数据预处理技术。它的作用是在输入图像的周围添加额外的像素，以保证图像能够被整除到指定的尺寸。图像缩放和补边的实现在`LetterBox`方法中，该方法的参数定义如下：

```cpp
float Letterbox(
    const cv::Mat &image,
    cv::Mat &out_image,
    const cv::Size &new_shape = cv::Size(640, 640),
    int stride = 32,
    const cv::Scalar &color = cv::Scalar(114, 114, 114),
    bool fixed_shape = false,
    bool scale_up = false);
```

该函数主要有以下几个参数，`image`是我们输入的原始图像，`out_image`是经过预处理后的输出图像。`new_shape`是需要缩放到的目标大小，**一般设置为`YOLOv5`模型输入的大小**，此处默认为$640 \times 640$. `Color`参数表示补边时所使用的颜色，其他参数并不是不是重点，我们可以默认将它们设为`false`.

```cpp
float Letterbox(...){
	...
    cv::Size shape = image.size();
    float r = std::min((float) new_shape.height / (float) shape.height, 
                       (float) new_shape.width / (float) shape.width);
    if (!scale_up) {
        r = std::min(r, 1.0f);
    }

    int new_unpad[2]{ (int) std::round((float) shape.width * r),
                     (int) std::round((float) shape.height * r)};
	...
}
```

在`letter_box`函数中，r是$\frac{新的高度}{新的宽度}$和$\frac{旧的高度}{旧的宽度}$两个比值的较小值，它的作用是为了在之后的`resize`中让新的图像保持合适的横纵比，防止缩放后的图像发生扭曲变形的情况。而`new_unpad`则是图像本身（不包含补边）缩放后的新形状大小，它的目的在于保持图像的横纵比不变。

<img src="https://i.imgur.com/05dXVqN.png" alt="image-20230906210442202" style="zoom:80%;" />

根据上图所示，蓝线部分代表经过`resize`后的图像大小，即`new_unpad`的大小；黄线部分则表示输出图像的整体大小。所以我们可以知道，由于`resize`后的图像不一定与输出大小匹配，比如我们将输入图像`resize`至 $520\times 640 $以保持比例，与目标大小$640\times 640$不符，这种情况下我们就需要进行图像补边，以将图像填充至指定的目标大小。

```cpp
float Letterbox(...){
    float dw = new_shape.width - new_unpad[0];
    float dh = new_shape.height - new_unpad[1];

    if (!fixed_shape) {
        dw = (float) ((int) dw % stride);
        dh = (float) ((int) dh % stride);
    }

    dw /= 2.0f;
    dh /= 2.0f;

    int top = int(std::round(dh - 0.1f));
    int bottom = int(std::round(dh + 0.1f));
    int left = int(std::round(dw - 0.1f));
    int right = int(std::round(dw + 0.1f));
    cv::copyMakeBorder(tmp, out_image, top, bottom, left, right, cv::BORDER_CONSTANT, color);

```

根据上图，`dw`和`dh`就是需要补边的大小，也就是蓝线和黄线之间的长度差距。我们使用 `cv::copyMakeBorder`对这个差距进行填充，填充的颜色是由`color`参数所指定的。经过图像缩放和补边处理，原图像的大小调整为$640 \times 640$，在保持原有图像横纵比的前提下，对图像进行了等比例缩放，图像的空余部分用边缘像素进行了补边，以填充至输出图像的大小。

### 颜色空间归一化

这样做的目的是为了将图像像素值映射到0-1之间，这可以减少量化误差，也使图像更容易被神经网络处理。因为原始的`RGB`值通常是0-255范围的整数，范围较大，通过归一化可以把值映射到一个更小的区间，有利于网络训练。

```cpp
kuiper_infer::sftensor PreProcessImage(const cv::Mat& image,
                                       const int32_t input_h,
                                       const int32_t input_w){
	...
    ...
    cv::Mat rgb_image;
    cv::cvtColor(out_image, rgb_image, cv::COLOR_BGR2RGB);

    cv::Mat normalize_image;
    rgb_image.convertTo(normalize_image, CV_32FC3, 1. / 255.);
```

### 颜色空间转换

正如上文所提到的，这里的颜色空间转换是将图像像素分布从`RGBRGBRGB` 转换到 `RRRGGGBBB`。也就是说，将像素的存储格式从原来的`HWC`(即高度-宽度-通道)转换为`CHW`(即通道-高度-宽度)。这个转换可以让之后的运算更加方便和高效，因为许多深度学习模型都采用`CHW`格式作为输入。

```cpp
kuiper_infer::sftensor PreProcessImage(const cv::Mat& image,
                                       const int32_t input_h,
                                       const int32_t input_w){
	...
    ...
    std::vector<cv::Mat> split_images;
    cv::split(normalize_image, split_images);
    assert(split_images.size() == input_c);

    std::shared_ptr<Tensor<float>> input =
        std::make_shared<Tensor<float>>(input_c, input_h, input_w);
    input->Fill(0.f);

    int index = 0;
    int offset = 0;
    for (const auto& split_image : split_images) {
        assert(split_image.total() == input_w * input_h);
        const cv::Mat& split_image_t = split_image.t();
        memcpy(input->slice(index).memptr(), split_image_t.data,
               sizeof(float) * split_image.total());
        index += 1;
        offset += split_image.total();
    }
```

首先，我们使用`cv::split`将图像的`RGB`三个通道拆分开来，分别存储到`split_images`数组中，同时准备好一个空的`input`张量来存储转换后的结果。

然后，再使用for循环来处理存储了拆分通道的`split_images`数组，在每次循环中，我们取出其中一个图像通道，由于`opencv`使用的是行主序存储，而我们是列主序存储，所以要对每个`split_image`进行转置.t()操作。经过这样的三次循环，我们将每个通道的数据（也就是R通道、G通道和B通道）逐通道的复制到`input`张量中，这样就可以实现从`HWC`格式到`CHW`格式。

**至此，我们就完成了图像预处理的全部流程。**

## 预处理函数的调用

预处理函数的调用过程在`YoloDemo`函数中，`YoloDemo`的参数定义如下：

```c++
void YoloDemo(const std::vector<std::string> &image_paths,
              const std::string &param_path,
              const std::string &bin_path,
              const uint32_t batch_size) 
```

`image_paths`为图片的路径列表，其中的图片数量与`batch_size`的数量一致。`param_path`表示模型参数文件的路径，`bin_path`表示模型的持久化文件路径。

```cpp
void YoloDemo(...){
    using namespace kuiper_infer;
    const int32_t input_h = 640;
    const int32_t input_w = 640;
	..
    std::vector<sftensor> inputs;
    for (uint32_t i = 0; i < batch_size; ++i) {
        const auto &input_image = cv::imread(image_paths.at(i));
        sftensor input = PreProcessImage(input_image, input_h, input_w);
        assert(input->rows() == 640);
        assert(input->cols() == 640);
        inputs.push_back(input);
    }  
}
```

`inputs`是一个数组类型的变量，用于存储经过预处理的输入图像张量，其长度与`batch_size`相同。也就是说，从另一方面来看，`inputs`就是一个长度为`batch_size`的`Yolo`模型输入张量数组。

## 模型的加载

载入`Yolo`模型的方法如下：

```cpp
RuntimeGraph graph(param_path, bin_path);
graph.Build("pnnx_input_0", "pnnx_output_0");
```

但是由于我们还没有实现 `Yolov5` 中的所有算子，会出现一些算子找不到的错误。在下面的过程中，我们需要逐步补充实现那些缺失的算子，以解决这些问题。

```shell
COULD NOT CREATE A LOGGINGFILE 20230321-131652.4249!F20230321 13:16:52.668184  4249 layer_factory.cpp:29] Can not find the layer type: nn.SiLU
*** Check failure stack trace: ***
```

从报错信息可以看出，我们没有实现`SiLU`这个算子，所以我们需要进行补充实现。

## 编写SiLU算子

`SiLU`算子的数学公式如下：
$$
SiLU(x) = x \times sigmoid(x)=\frac{x}{1+e^{-x}}
$$
这个算子本质上是将`sigmoid`函数和输入值$x$进行相乘，以下是它的具体实现：

```cpp
StatusCode SiLULayer::Forward(
    const std::vector<std::shared_ptr<Tensor<float>>>& inputs,
    std::vector<std::shared_ptr<Tensor<float>>>& outputs) {
    if (inputs.empty()) {
        LOG(ERROR) << "The input tensor array in the silu layer is empty";
        return StatusCode::kInferInputsEmpty;
    }

    if (outputs.empty()) {
        LOG(ERROR) << "The output tensor array in the silu layer is empty";
        return StatusCode::kInferOutputsEmpty;
    }

    if (inputs.size() != outputs.size()) {
        LOG(ERROR) << "The input and output tensor array size of the silu "
            "layer do not match";
        return StatusCode::kInferInOutSizeMismatch;
    }
```

在以上的代码中，`Forwards`函数的对输入张量和输出张量进行检查，检查它们是否为空以及长度是否相等。

```cpp
  const uint32_t batch_size = inputs.size();
  for (uint32_t i = 0; i < batch_size; ++i) {
    const std::shared_ptr<Tensor<float>> &input = inputs.at(i);
    CHECK(input == nullptr || !input->empty()) << "The input feature map of silu layer is empty!";

    std::shared_ptr<Tensor<float>> output = outputs.at(i);
    ...
    ...
    output->set_data(input->data());
    output->Transform([](const float value) {
      return value / (1.f + expf(-value));
    });
  }
  return InferStatus::kInferSuccess;
}
```

在上述函数中，我们对`batch_size`个批次的数据进行处理。首先，我们获取当前的输入张量`input`，并将其拷贝到`output`张量中。然后，我们对`output`中的数据进行处理，该处理方式在`Transform`函数中定义，处理方法和上述公式定义的相同，也就是$\frac{x}{1+e^{-x}}$.

```cpp
LayerRegistererWrapper kSiluGetInstance("nn.SiLU", SiLULayer::GetInstance);
```

完成算子的编写后，我们使用算子注册功能将`SiLU`的实现注册到全局。

## 编写Concat算子

`Concat`算子的实现位于cat.cpp文件中（这个文件名确实有点奇怪），其功能是将多个张量**沿着通道维**进行拼接。下面我们将使用图例和代码相结合的方式来说明其实现过程。

```cpp
StatusCode CatLayer::Forward(
    const std::vector<std::shared_ptr<Tensor<float>>>& inputs,
    std::vector<std::shared_ptr<Tensor<float>>>& outputs) {
  if (inputs.empty()) {
    LOG(ERROR) << "The input tensor array in the cat layer is empty";
    return StatusCode::kInferInputsEmpty;
  }

  if (outputs.empty()) {
    LOG(ERROR) << "The output tensor array in the cat layer is empty";
    return StatusCode::kInferOutputsEmpty;
  }

  if (dim_ != 1 && dim_ != -3) {
    LOG(ERROR) << "The dimension parameter of cat layer is error";
    return StatusCode::kInferParameterError;
  }

  const uint32_t output_size = outputs.size();
  if (inputs.size() % output_size != 0) {
    LOG(ERROR)
        << "The input and output tensor array size of cat layer do not match";
    return StatusCode::kInferInOutSizeMismatch;
  }
```

在进行`concat`算子的实现之前，首先需要检查输入张量数组和输出张量数组是否为空，并且确保输入张量的个数是输出张量个数的整数倍。让我们以一个例子来说明：**我们想要将多个张量进行拼接，按照每4个张量为一组进行拼接，那么输出张量的个数一定可以被4整除。这样就满足了输入张量个数是输出张量个数的整数倍的条件。**

在以上的代码中`input_size`是参加拼接的张量数量，`output_size`是**按组拼接**(通道维)后得到的张量数量，所以它们存在可以被整数除的关系。

```cpp
 for (uint32_t i = 0; i < outputs.size(); ++i) {
    std::shared_ptr<Tensor<float>> output = outputs.at(i);
    uint32_t start_channel = 0;

    for (uint32_t j = i; j < inputs.size(); j += output_size) {
      const std::shared_ptr<Tensor<float>>& input = inputs.at(j);
      ...
      uint32_t in_rows = input->rows();
      uint32_t in_cols = input->cols();
      const uint32_t in_channels = input->channels();
      ...
      const uint32_t plane_size = in_rows * in_cols;
      memcpy(output->raw_ptr(start_channel * plane_size), input->raw_ptr(),
             sizeof(float) * plane_size * in_channels);
      start_channel += input->channels();
    }
```

在上述代码中，我们按照通道维度逐组拼接 `input` 张量，每组张量的数量为$input\_size/output\_size$。因此，最终得到的 `output` 张量总数为 `output size`。换句话说，`input` 张量的数量和维度为 $input\_size \times input\_channel \times row \times col$.而拼接后的 `output` 张量的数量和维度为 $output\_size \times output\_channel \times row \times col$.

其中，$output\_channel = input\_channel \times 每组张量数$。从第 13 行代码可以看出，我们逐个输入张量按照通道维度拼接到 `output` 张量上，$start\_channel \times plane\_size$ 表示当前的实际拼接开始位置（在通道维度上）。

## 编写UpSample算子

这是一个上采样算子，它的作用是将输入的大小（宽度和高度）放大到指定的倍数`scale`。在这里，我们使用了最近邻插值的方法来进行上采样，即通过复制最近的像素值来进行放大。

这个算子的实现总体而言比较简单。如下图所示，**输出图像中**任意一个位于(0, 0)到(3, 3)之间的像素点，在`scale`等于4的情况下，都会**拷贝自输入图像中(0, 0)位置像素的值**，因为：
$$
\, x\div\,scale=0, \, y\div scale=0 \quad x\in(0,3),y\in(0,3),scale=4
$$
它的实现放在`upsample.cpp`中，具体到代码如下：

```cpp
StatusCode UpSampleLayer::Forward(
    const std::vector<std::shared_ptr<Tensor<float>>>& inputs,
    std::vector<std::shared_ptr<Tensor<float>>>& outputs){
    const uint32_t batch_size = inputs.size();
    #pragma omp parallel for num_threads(batch_size)
    for (uint32_t i = 0; i < batch_size; ++i) {
        const arma::fcube& input_data = inputs.at(i)->data();

        auto& output_data = output->data();
        CHECK(output_data.n_rows == std::floor(input_data.n_rows * scale_h_))<< ...
        CHECK(output_data.n_cols == std::floor(input_data.n_cols * scale_w_))<< ...
        CHECK(input_data.n_slices == output_data.n_slices) << ...

        const uint32_t channels = input_data.n_slices;
```

首先，代码获取了输入张量空间和输出张量空间，分别命名为`input`和`output`。接下来，代码会检查输出张量空间是否足够容纳经过上采样后的输入数据，即将输入数据`input`的长和宽都乘以`scale`倍数后的大小再比较是否和输出张量大小一致。
```cpp
for (uint32_t c = 0; c < channels; ++c) {
    const arma::fmat& input_channel = input_data.slice(c);
    arma::fmat& output_channel = output_data.slice(c);

    const uint32_t input_w = input_channel.n_cols;
    const uint32_t input_h = input_channel.n_rows;

    for (uint32_t w = 0; w < input_w; ++w) {
        const float* input_col_ptr = input_channel.colptr(w);
        const uint32_t scaled_w = w * static_cast<uint32_t>(scale_w_);
        for (uint32_t sw = 0; sw < static_cast<uint32_t>(scale_w_); ++sw) {
            if (scaled_w + sw >= output_w) {
                continue;
            }
            float* output_col_ptr = output_channel.colptr(scaled_w + sw);
            for (uint32_t h = 0; h < input_h; ++h) {
                const uint32_t scaled_h = h * static_cast<uint32_t>(scale_h_);
                float* output_ptr = output_col_ptr + scaled_h;
                float input_value = *(input_col_ptr + h);
                for (uint32_t sh = 0; sh < static_cast<uint32_t>(scale_h_);
                     ++sh) {
                    if (scaled_h + sh < output_h) {
                        *(output_ptr + sh) = input_value;
                    }
                }
            }
        }
    }
}
```
接下来我们会获取输入张量的其中一个维度，命名为`input_channel`，然后我们对`input_channel`进行循环遍历。

在遍历过程中，并将`input_channel`上的坐标 (w, h) 分别乘以`scale_h_`和`scale_w_`，得到输出张量上的坐标(scale_h, scale_w)，并将输入张量中该位置(w, h)的值拷贝到输出张量的该坐标上，从而完成**最近邻采样**的过程。

![](https://i.imgur.com/FhxC5Uo.png)

从上图例中，我们可以看出黄色是输入通道中的其中一个元素，它的坐标是(w, h)。它的值会被拷贝到右侧输出通道中蓝色区域的位置（此时scale_h和scale_w均为3）。

##  编写Yolo Head算子

Yolov5的网络结构包含了两个主要部分：特征提取网络和`Yolo head`. 其中，`Yolo head`是整个网络的最后几层，它负责根据特征提取网络提取的特征图进行目标检测的预测，它主要起到了以下几点作用：

1. 边界框回归：`Yolo head`通过对特征图进行卷积操作，预测出每个候选框的位置和大小。这些候选框表示图像中可能存在的目标物体的位置信息。
2. 类别预测：除了预测边界框的位置信息，`Yolo head`还负责对每个候选框进行类别预测。它会输出一个包含各个类别得分的向量，用于表示每个候选框属于各个类别的可能性。
3. 概率计算：`Yolo head`会对每个候选框的类别得分进行概率计算，以确定最终的检测结果。通过计算类别得分与边界框位置得分的乘积，可以得到每个候选框属于某个类别的概率，从而选择最可能的类别并过滤掉低概率的检测结果。

Yolov5项目中关于这部分的代码如下，见[Yolov5项目](https://github.com/ultralytics/yolov5/blob/a6659d05051e01c8feca7ecb348c1cce7d67aaaa/models/yolo.py#L38)。

```python
def forward(self, x):
    z = []  # inference output
    for i in range(self.nl):
        x[i] = self.m[i](x[i])  # conv
        bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)
        x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()

        if not self.training:  # inference
            省略...
        else:  # Detect (boxes only)
            xy, wh, conf = x[i].sigmoid().split((2, 2, self.nc + 1), 4)
            xy = (xy * 2 + self.grid[i]) * self.stride[i]  # xy
            wh = (wh * 2) ** 2 * self.anchor_grid[i]  # wh
            y = torch.cat((xy, wh, conf), 4)
            z.append(y.view(bs, self.na * nx * ny, self.no))

```

在第4-6行中，主要对输入的特征图进行卷积运算，并对卷积操作得到的结果`x[i]`进行形状和维度上的调整，以方便后续的处理流程。

在第11行中，我们再对结果`x[i]`进行`sigmoid`运算，并将运算后的特征在最后一维进行切分，得到`xy`、`wh`和`conf`, 分别表示目标框的中心位置、宽高和置信度。然后我们将这三个部分的输出**沿最后一维进行连接**（`concat`），得到最终的结果`y`.

```cpp
std::vector<std::vector<sftensor>> stage_outputs(stages);
for (uint32_t stage = 0; stage < stages; ++stage) {
    const std::vector<std::shared_ptr<Tensor<float>>>& stage_input =
        batches.at(stage);

    CHECK(stage_input.size() == batch_size)
        << "The number of stage input in the yolo detect layer should be equal "
        "to batch size";

    std::vector<std::shared_ptr<Tensor<float>>> stage_output(batch_size);
    const auto status =
        this->conv_layers_.at(stage)->Forward(stage_input, stage_output);
    ...
```

在`yolo_detect.cpp`文件中，我们将逐步分析并还原以上的过程。在以上的代码中，我们还原了Python代码中的第6-7行**对输入特征图进行卷积运算**的操作，并将最后的计算结果存放到`stage_output`中，随后我们对`stage_output`数组中的每个特征进行处理。

```cpp
for (uint32_t b = 0; b < batch_size; ++b) {
    const std::shared_ptr<Tensor<float>>& input = stage_output.at(b);
    CHECK(input != nullptr && !input->empty());
    const uint32_t nx = input->rows();
    const uint32_t ny = input->cols();
    input->Reshape({stages, uint32_t(classes_info), ny * nx}, true);
    const uint32_t size = input->size;
    input->Transform(
          [](const float value) { return 1.f / (1.f + expf(-value)); });
```

在这里，我们对卷积输出进行逐批次的处理。`input->Reshape`对应的是`x[i].view`函数，用于将张量重新调整形状为`(stages, classes_info, ny, nx)`，随后对`input`张量进行`sigmoid`计算（相当于Python代码中的`x[i].sigmoid()`）。

```cpp
std::shared_ptr<Tensor<float>> stages_tensor =
    TensorCreate<float>(batch_size, stages * nx * ny, uint32_t(num_classes_ + 5));
stage_tensors.push_back(stages_tensor);

for (uint32_t b = 0; b < batch_size; ++b) {
	...
    ...
    arma::fmat& x_stages = stages_tensor->slice(b);
    for (uint32_t na = 0; na < num_anchors_; ++na) {
        x_stages.submat(ny * nx * na, 0, ny * nx * (na + 1) - 1,
                        classes_info - 1) = input_data.slice(na).t();
    }
```

在Python代码中，经过`permute(0, 1, 3, 4, 2).contiguous()`处理后，`x[i]`的维度为`(self.na, ny, nx, self.no)`，其中`self.no = class_info`.而我们的`input`张量维度在**reshape**后仍是`(stages, classes_info, ny, nx)`，因此在这里我们需要**逐通道**对`input`张量进行**转置**以与`x[i]`的维度**保持一致**，并将数据存放到`x_stages`中。

```cpp
std::shared_ptr<Tensor<float>> stages_tensor =
    TensorCreate<float>(batch_size, stages * nx * ny, uint32_t(num_classes_ + 5));
stage_tensors.push_back(stages_tensor);


for (uint32_t b = 0; b < batch_size; ++b) {
    arma::fmat& x_stages = x_stages_tensor->slice(b);
	...
    ...
    // 此处的x_stages与python代码中的x[i]含义是相同的
    x_stages.submat(0, 0, x_stages.n_rows - 1, 1) =
        (xy * 2 + grids_[stage]) * strides_[stage];
    x_stages.submat(0, 2, x_stages.n_rows - 1, 3) =
        arma::pow((wh * 2), 2) % anchor_grids_[stage];
```

==经过对`input`进行**reshape**和转置，得到的`input`与Python代码中的`x[i]`相同。==

正如前文所述，变量`xy`代表矩阵`x_stages`的第0至1列，而变量`wh`代表矩阵`x_stages`的第2至3列。因此，我们可以方便地使用`submat`函数来提取相关变量，并完成与`stride`和`anchor`变量的乘法和幂次操作。在处理完一个批次的所有数据后，结果将会被放置到`stage_tensor`的相应位置中。

**对应的Python代码：**

```python
xy = (xy * 2 + self.grid[i]) * self.stride[i]
wh = (wh * 2) ** 2 * self.anchor_grid[i]
```

```cpp
uint32_t current_rows = 0;
arma::fcube f1(concat_rows, classes_info, batch_size);
for (std::shared_ptr<ftensor> stages_tensor : stage_tensors) {
    f1.subcube(current_rows, 0, 0, current_rows + stages_tensor->rows() - 1,
               classes_info - 1, batch_size - 1) = stages_tensor->data();
    current_rows += stages_tensor->rows();
}
```

在这里，`stages`变量代表不同的检测头，类似于`Yolov5`中的三个检测头。这些检测头用于适应不同大小和尺寸的物体检测，接下来，我们将重新拼接一个批次的输出，即三个检测头的输出，并将其存放到`f1`中。举个例子，对于上述提到的`stages`，每个`stage`的维度依次为`(1, 8, 19200, 85)`、`(1, 8, 4800, 85)`和`(1, 8, 1200, 85)`。当我们将它们拼接在一起后，得到的`f1`变量的维度将会是`(1, 25200, 85)`。这个维度将作为算子的最终输出。如果你对Yolov5模型很熟悉，你可能对这个数字有所敏感。

## Yolo模型的运行

